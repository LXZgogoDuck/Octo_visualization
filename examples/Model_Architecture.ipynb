{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from octo.model.octo_model import OctoModel\n",
    "import cv2\n",
    "import jax\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import mediapy\n",
    "import numpy as np\n",
    "\n",
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
    "ds = builder.as_dataset(split='train[:1]')\n",
    "\n",
    "# sample episode + resize to 256x256 (default third-person cam resolution)\n",
    "episode = next(iter(ds))\n",
    "steps = list(episode['steps'])\n",
    "images = [cv2.resize(np.array(step['observation']['image']), (256, 256)) for step in steps]\n",
    "\n",
    "# extract goal image & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[0]['observation']['natural_language_instruction'].numpy().decode()\n",
    "\n",
    "# visualize episode\n",
    "print(f'Instruction: {language_instruction}')\n",
    "mediapy.show_video(images, fps=10)\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "pred_actions, true_actions = [], []\n",
    "for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "    input_images = np.stack(images[step:step+WINDOW_SIZE])[None]\n",
    "    observation = {\n",
    "        'image_primary': input_images,\n",
    "        'timestep_pad_mask': np.full((1, input_images.shape[1]), True, dtype=bool)\n",
    "    }\n",
    "\n",
    "    # this returns *normalized* actions --> we need to unnormalize using the dataset statistics\n",
    "    actions = model.sample_actions(\n",
    "        observation,\n",
    "        task,\n",
    "        unnormalization_statistics=model.dataset_statistics[\"bridge_dataset\"][\"action\"],\n",
    "        rng=jax.random.PRNGKey(0)\n",
    "    )\n",
    "    actions = actions[0] # remove batch dim\n",
    "\n",
    "    pred_actions.append(actions)\n",
    "    final_window_step = step + WINDOW_SIZE - 1\n",
    "    true_actions.append(np.concatenate(\n",
    "        (\n",
    "            steps[final_window_step]['action']['world_vector'],\n",
    "            steps[final_window_step]['action']['rotation_delta'],\n",
    "            np.array(steps[final_window_step]['action']['open_gripper']).astype(np.float32)[None]\n",
    "        ), axis=-1\n",
    "    ))\n",
    "\n",
    "# create `task` dict\n",
    "task = model.create_tasks(goals={\"image_primary\": goal_image[None]})   # for goal-conditioned\n",
    "task = model.create_tasks(texts=[language_instruction])                  # for language conditioned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
