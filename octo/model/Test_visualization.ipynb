{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:16:44.279389Z",
     "end_time": "2025-02-25T04:16:57.851265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45e8cc38544e4591b626fa24d25a6294"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "from octo.model.octo_model import OctoModel\n",
    "model = OctoModel.load_pretrained(\"hf://rail-berkeley/octo-small-1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BRIDGE dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 04:16:57.876399: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Place the can to the left of the pot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 04:17:03.409 python[2988:22304] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-25 04:17:03.409 python[2988:22304] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_datasets as tfds\n",
    "import cv2\n",
    "import jax\n",
    "from PIL import Image\n",
    "import mediapy as mp\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "### Load the BRIDGE Dataset\n",
    "print(\"Loading BRIDGE dataset...\")\n",
    "builder = tfds.builder_from_directory(builder_dir=\"gs://gresearch/robotics/bridge/0.1.0/\")\n",
    "ds = builder.as_dataset(split=\"train[:1]\")  # Load first episode\n",
    "\n",
    "# Extract a single episode\n",
    "episode = next(iter(ds))\n",
    "steps = list(episode[\"steps\"])\n",
    "\n",
    "# Resize images to 256x256 (default for Octo model)\n",
    "images = [cv2.resize(np.array(step[\"observation\"][\"image\"]), (256, 256)) for step in steps]\n",
    "\n",
    "# Extract goal image (last frame) & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[0][\"observation\"][\"natural_language_instruction\"].numpy().decode()\n",
    "\n",
    "print(f\"Instruction: {language_instruction}\")\n",
    "for img in images:\n",
    "    cv2.imshow(\"Episode Frame\", img)\n",
    "    cv2.waitKey(100)  # Wait 100ms per frame\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:16:57.852350Z",
     "end_time": "2025-02-25T04:17:07.056385Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Octo model checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7938f8441134840b123b3567449bbe5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'tasks' is missing items compared to example_batch: {'image_wrist', 'pad_mask_dict/timestep', 'pad_mask_dict/image_wrist', 'timestep'}\n"
     ]
    }
   ],
   "source": [
    "### Load the Pretrained Octo Model\n",
    "print(\"Loading Octo model checkpoint...\")\n",
    "model = OctoModel.load_pretrained(\"hf://rail-berkeley/octo-base-1.5\")\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "task = model.create_tasks(goals={\"image_primary\": goal_image[None]})   # for goal-conditioned\n",
    "task = model.create_tasks(texts=[language_instruction])                # for language conditioned"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:17:07.056281Z",
     "end_time": "2025-02-25T04:17:12.264696Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]WARNING:root:'observations' is missing items compared to example_batch: {'pad_mask_dict/image_primary', 'timestep', 'image_wrist', 'pad_mask_dict/timestep', 'pad_mask_dict/image_wrist', 'task_completed'}\n",
      "WARNING:root:No pad_mask_dict found. Nothing will be masked.\n",
      "WARNING:root:Skipping observation tokenizer: obs_wrist\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:24<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_actions, true_actions, attention_maps_per_step = [], [], []\n",
    "for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "    input_images = np.stack(images[step:step+WINDOW_SIZE])[None]\n",
    "    observation = {\n",
    "        'image_primary': input_images,\n",
    "        'timestep_pad_mask': np.full((1, input_images.shape[1]), True, dtype=bool)\n",
    "    }\n",
    "\n",
    "    # Get both predicted actions and attention maps\n",
    "    actions, attention_maps = model.sample_actions(\n",
    "        observation,\n",
    "        task,\n",
    "        unnormalization_statistics=model.dataset_statistics[\"bridge_dataset\"][\"action\"],\n",
    "        rng=jax.random.PRNGKey(0)\n",
    "    )\n",
    "\n",
    "    # Store predicted actions\n",
    "    pred_actions.append(actions[0])\n",
    "\n",
    "    # Store attention maps (for all layers & heads at this step)\n",
    "    attention_maps_per_step.append(attention_maps)\n",
    "\n",
    "    # Store true actions\n",
    "    final_window_step = step + WINDOW_SIZE - 1\n",
    "    true_actions.append(np.concatenate(\n",
    "        (\n",
    "            steps[final_window_step]['action']['world_vector'],\n",
    "            steps[final_window_step]['action']['rotation_delta'],\n",
    "            np.array(steps[final_window_step]['action']['open_gripper']).astype(np.float32)[None]\n",
    "        ), axis=-1\n",
    "    ))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:17:12.266469Z",
     "end_time": "2025-02-25T04:17:37.182245Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def overlay_attention_on_image(image, attn_map, patch_size=16):\n",
    "    \"\"\"\n",
    "    Overlays attention heatmap on an input image.\n",
    "\n",
    "    Args:\n",
    "        image: (H, W, C) - The original image.\n",
    "        attn_map: (num_heads, num_tokens) - Attention scores for image tokens.\n",
    "        patch_size: The patch size used in tokenization (default = 16).\n",
    "\n",
    "    Returns:\n",
    "        List of overlayed images (one per head).\n",
    "    \"\"\"\n",
    "    num_heads, num_patches = attn_map.shape\n",
    "    grid_size = int(np.sqrt(num_patches))  # E.g., 256 tokens â†’ 16x16 grid\n",
    "    overlayed_images = []\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        # Normalize attention map per head\n",
    "        attn_grid = attn_map[head].reshape((grid_size, grid_size))\n",
    "        attn_grid = (attn_grid - attn_grid.min()) / (attn_grid.max() - attn_grid.min())\n",
    "\n",
    "        # Upsample the attention map to match image size\n",
    "        attn_heatmap = zoom(attn_grid, (patch_size, patch_size), order=1)\n",
    "\n",
    "        # Convert attention map to heatmap\n",
    "        attn_colormap = cv2.applyColorMap((attn_heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "        # Resize the heatmap to match the image resolution\n",
    "        attn_colormap = cv2.resize(attn_colormap, (image.shape[1], image.shape[0]))\n",
    "\n",
    "        # Blend the heatmap with the original image\n",
    "        overlayed_image = cv2.addWeighted(image, 0.6, attn_colormap, 0.4, 0)\n",
    "        overlayed_images.append(overlayed_image)\n",
    "\n",
    "    return overlayed_images  # List of images, one per head"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:17:37.192243Z",
     "end_time": "2025-02-25T04:17:37.209595Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def visualize_attention_maps(attention_maps_per_step, images, save_path=\"attention_visualization\"):\n",
    "    \"\"\"\n",
    "    Visualizes and saves attention maps overlaid on images.\n",
    "\n",
    "    Args:\n",
    "        attention_maps_per_step: List of attention maps extracted from OctoModel.\n",
    "        images: List of original images used for inference.\n",
    "        save_path: Directory to save visualized attention maps.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    num_steps = len(attention_maps_per_step)\n",
    "    num_layers = len(attention_maps_per_step[0][\"all\"])  # Assuming all steps have same layers\n",
    "\n",
    "    for step in tqdm.trange(num_steps, desc=\"Visualizing Attention\"):\n",
    "        image = images[step]\n",
    "\n",
    "        for layer_idx in range(num_layers):  # Iterate over all layers\n",
    "            attn_map = attention_maps_per_step[step][\"all\"][layer_idx][0] # Shape: (num_heads, image_token_num)\n",
    "\n",
    "            # Verify shape is correct\n",
    "            if attn_map.ndim != 2:\n",
    "                print(f\"Unexpected shape for attention map at step {step}, layer {layer_idx}: {attn_map.shape}\")\n",
    "                continue\n",
    "\n",
    "            # Get overlaid images per head\n",
    "            overlayed_images = overlay_attention_on_image(image, attn_map)\n",
    "\n",
    "            # Plot all attention heads in a row\n",
    "            num_heads = attn_map.shape[0]\n",
    "            fig, axes = plt.subplots(1, num_heads, figsize=(20, 5))\n",
    "\n",
    "            for head_idx in range(num_heads):\n",
    "                axes[head_idx].imshow(overlayed_images[head_idx])\n",
    "                axes[head_idx].set_title(f\"Layer {layer_idx} - Head {head_idx}\")\n",
    "                axes[head_idx].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(save_path, f\"step_{step}_layer_{layer_idx}.png\"))\n",
    "            plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:17:37.202611Z",
     "end_time": "2025-02-25T04:17:37.210615Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing Attention: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [03:39<00:00,  5.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run visualization function\n",
    "visualize_attention_maps(attention_maps_per_step, images, save_path=\"attention_results\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:17:37.209872Z",
     "end_time": "2025-02-25T04:21:16.405577Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ Generating video and GIF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 444/444 [00:07<00:00, 55.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as: output/attention_video.mp4\n",
      "GIF saved as: output/attention_video.gif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_attention_video(image_folder=\"attention_results\", output_dir=\"output\", fps=5):\n",
    "    \"\"\"\n",
    "    Creates a video and a GIF from saved attention map images.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): Path to the folder containing attention images.\n",
    "        output_dir (str): Directory to save video and GIF.\n",
    "        fps (int): Frames per second for the video.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_video = os.path.join(output_dir, \"attention_video.mp4\")\n",
    "    output_gif = os.path.join(output_dir, \"attention_video.gif\")\n",
    "\n",
    "    # Get list of attention images (sorted by step)\n",
    "    images = sorted([img for img in os.listdir(image_folder) if img.endswith(\".png\")])\n",
    "\n",
    "    if not images:\n",
    "        print(\"No images found in the folder! Check if visualize_attention_maps() was run.\")\n",
    "        return\n",
    "\n",
    "    # Load first image to get video dimensions\n",
    "    sample_image = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    if sample_image is None:\n",
    "        print(\"Error loading sample image. Check if images exist in the folder.\")\n",
    "        return\n",
    "\n",
    "    height, width, _ = sample_image.shape\n",
    "\n",
    "    # Create MP4 video writer\n",
    "    video_writer = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    # Create GIF writer\n",
    "    gif_images = []\n",
    "\n",
    "    print(\"ðŸŽ¥ Generating video and GIF...\")\n",
    "    for img_name in tqdm(images, desc=\"Processing Frames\"):\n",
    "        img_path = os.path.join(image_folder, img_name)\n",
    "        frame = cv2.imread(img_path)\n",
    "\n",
    "        if frame is None:\n",
    "            print(f\"Warning: Could not read image {img_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Write frame to video\n",
    "        video_writer.write(frame)\n",
    "\n",
    "        # Convert BGR to RGB for GIF\n",
    "        gif_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        gif_images.append(gif_frame)\n",
    "\n",
    "    # Release video writer\n",
    "    video_writer.release()\n",
    "\n",
    "    # Save GIF using imageio\n",
    "    imageio.mimsave(output_gif, gif_images, fps=fps)\n",
    "\n",
    "    print(f\"Video saved as: {output_video}\")\n",
    "    print(f\"GIF saved as: {output_gif}\")\n",
    "\n",
    "# Run the function\n",
    "create_attention_video(image_folder=\"attention_results\", output_dir=\"output\", fps=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:21:16.408272Z",
     "end_time": "2025-02-25T04:22:06.557648Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing Mean Attention: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:01<00:00, 20.91it/s]\n"
     ]
    }
   ],
   "source": [
    "def overlay_attention_on_image(image, attn_map, patch_size=16, title=\"\"):\n",
    "\n",
    "    num_patches = attn_map.shape[-1]\n",
    "    grid_size = int(np.sqrt(num_patches))  # E.g., 256 tokens â†’ 16x16 grid\n",
    "\n",
    "    # Ensure attention map is (num_tokens,)\n",
    "    if attn_map.ndim == 2:\n",
    "        attn_map = attn_map.mean(axis=0)  # Take mean across heads\n",
    "\n",
    "    # Reshape into 2D grid\n",
    "    attn_grid = attn_map.reshape((grid_size, grid_size))\n",
    "    attn_grid = (attn_grid - attn_grid.min()) / (attn_grid.max() - attn_grid.min())\n",
    "\n",
    "    # Upsample the attention map to match image size\n",
    "    attn_heatmap = zoom(attn_grid, (patch_size, patch_size), order=1)\n",
    "\n",
    "    # Convert attention map to heatmap\n",
    "    attn_colormap = cv2.applyColorMap((attn_heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Resize to match image\n",
    "    attn_colormap = cv2.resize(attn_colormap, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Blend the heatmap with the original image\n",
    "    overlayed_image = cv2.addWeighted(image, 0.6, attn_colormap, 0.4, 0)\n",
    "\n",
    "    return overlayed_image\n",
    "\n",
    "def visualize_mean_max_attention(attention_maps_per_step, images, save_path=\"attention_results_mean_max\"):\n",
    "    \"\"\"\n",
    "    Visualizes and saves mean and max attention maps overlaid on images.\n",
    "\n",
    "    Args:\n",
    "        attention_maps_per_step: List of attention maps extracted from OctoModel.\n",
    "        images: List of original images used for inference.\n",
    "        save_path: Directory to save visualized attention maps.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    num_steps = len(attention_maps_per_step)\n",
    "    num_layers = len(attention_maps_per_step[0][\"all\"])  # Assuming all steps have the same layers\n",
    "\n",
    "    for step in tqdm(range(num_steps), desc=\"Visualizing Mean & Max Attention\"):\n",
    "        image = images[step]\n",
    "\n",
    "        for layer_idx in range(num_layers):  # Iterate over layers\n",
    "            attn_map = attention_maps_per_step[step][\"all\"][layer_idx]  # Shape: (num_heads, image_token_num)\n",
    "\n",
    "            # Fix batch dimension issue if necessary\n",
    "            if attn_map.ndim == 3 and attn_map.shape[0] == 1:\n",
    "                attn_map = attn_map[0]  # Remove batch dim (now: (num_heads, image_token_num))\n",
    "\n",
    "            if attn_map.ndim != 2:\n",
    "                print(f\"Unexpected shape at step {step}, layer {layer_idx}: {attn_map.shape}\")\n",
    "                continue\n",
    "\n",
    "            # Compute mean & max across heads\n",
    "            mean_attn = attn_map.mean(axis=0)  # (image_token_num,)\n",
    "            max_attn = attn_map.max(axis=0)  # (image_token_num,)\n",
    "\n",
    "            # Overlay attention on image\n",
    "            mean_overlay = overlay_attention_on_image(image, mean_attn, title=f\"Step {step} - Layer {layer_idx} (Mean)\")\n",
    "            max_overlay = overlay_attention_on_image(image, max_attn, title=f\"Step {step} - Layer {layer_idx} (Max)\")\n",
    "\n",
    "            # Save images\n",
    "            cv2.imwrite(os.path.join(save_path, f\"step_{step}_layer_{layer_idx}_mean.png\"), mean_overlay)\n",
    "            cv2.imwrite(os.path.join(save_path, f\"step_{step}_layer_{layer_idx}_max.png\"), max_overlay)\n",
    "\n",
    "def create_attention_video(image_folder, output_video, fps=5):\n",
    "    \"\"\"\n",
    "    Creates a video from saved attention map images.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): Path to the folder containing attention images.\n",
    "        output_video (str): Output filename for the video.\n",
    "        fps (int): Frames per second for the video.\n",
    "    \"\"\"\n",
    "    images = sorted([img for img in os.listdir(image_folder) if img.endswith(\".png\")])\n",
    "    if not images:\n",
    "        print(f\"No images found in {image_folder}\")\n",
    "        return\n",
    "\n",
    "    sample_image = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, _ = sample_image.shape\n",
    "\n",
    "    video_writer = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(image_folder, img_name)\n",
    "        frame = cv2.imread(img_path)\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved as: {output_video}\")\n",
    "\n",
    "def run_visualization(attention_maps_per_step, images):\n",
    "    visualize_mean_max_attention(attention_maps_per_step, images, save_path=\"attention_results_mean_max\")\n",
    "    create_attention_video(\"attention_results_mean_max\", \"mean_attention_video.mp4\", fps=2)\n",
    "    create_attention_video(\"attention_results_mean_max\", \"max_attention_video.mp4\", fps=2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:40:44.410976Z",
     "end_time": "2025-02-25T04:40:48.108585Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing Max Attention: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:01<00:00, 20.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as: max_attention_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# def visualize_max_attention(attention_maps_per_step, images, save_path=\"attention_results_max\"):\n",
    "#     \"\"\"\n",
    "#     Visualizes and saves mean and max attention maps overlaid on images.\n",
    "#\n",
    "#     Args:\n",
    "#         attention_maps_per_step: List of attention maps extracted from OctoModel.\n",
    "#         images: List of original images used for inference.\n",
    "#         save_path: Directory to save visualized attention maps.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(save_path, exist_ok=True)\n",
    "#\n",
    "#     num_steps = len(attention_maps_per_step)\n",
    "#     num_layers = len(attention_maps_per_step[0][\"all\"])  # Assuming all steps have the same layers\n",
    "#\n",
    "#     for step in tqdm(range(num_steps), desc=\"Visualizing Max Attention\"):\n",
    "#         image = images[step]\n",
    "#\n",
    "#         for layer_idx in range(num_layers):  # Iterate over layers\n",
    "#             attn_map = attention_maps_per_step[step][\"all\"][layer_idx]  # Shape: (num_heads, image_token_num)\n",
    "#\n",
    "#             # Fix batch dimension issue if necessary\n",
    "#             if attn_map.ndim == 3 and attn_map.shape[0] == 1:\n",
    "#                 attn_map = attn_map[0]  # Remove batch dim (now: (num_heads, image_token_num))\n",
    "#\n",
    "#             if attn_map.ndim != 2:\n",
    "#                 print(f\"Unexpected shape at step {step}, layer {layer_idx}: {attn_map.shape}\")\n",
    "#                 continue\n",
    "#\n",
    "#             max_attn = attn_map.max(axis=0)  # (image_token_num,)\n",
    "#             max_overlay = overlay_attention_on_image(image, max_attn, title=f\"Step {step} - Layer {layer_idx} (Max)\")\n",
    "#             cv2.imwrite(os.path.join(save_path, f\"step_{step}_layer_{layer_idx}_max.png\"), max_overlay)\n",
    "#\n",
    "# def create_attention_video(image_folder, output_video, fps=5):\n",
    "#\n",
    "#     images = sorted([img for img in os.listdir(image_folder) if img.endswith(\".png\")])\n",
    "#     if not images:\n",
    "#         print(f\"No images found in {image_folder}\")\n",
    "#         return\n",
    "#\n",
    "#     sample_image = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "#     height, width, _ = sample_image.shape\n",
    "#\n",
    "#     video_writer = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "#\n",
    "#     for img_name in images:\n",
    "#         img_path = os.path.join(image_folder, img_name)\n",
    "#         frame = cv2.imread(img_path)\n",
    "#         video_writer.write(frame)\n",
    "#\n",
    "#     video_writer.release()\n",
    "#     print(f\"Video saved as: {output_video}\")\n",
    "#\n",
    "# def run_visualization(attention_maps_per_step, images):\n",
    "#     visualize_max_attention(attention_maps_per_step, images, save_path=\"attention_results_max\")\n",
    "#     create_attention_video(\"attention_results_max\", \"max_attention_video.mp4\", fps=2)\n",
    "#\n",
    "# run_visualization(attention_maps_per_step, images)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-25T04:51:52.385506Z",
     "end_time": "2025-02-25T04:51:55.049956Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## extract attention from task-related words to image tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "# from scipy.ndimage import zoom\n",
    "#\n",
    "# def extract_task_to_image_attention(attention_maps_per_step, task_token_idx, image_token_indices):\n",
    "#     \"\"\"\n",
    "#     Extracts attention weights from task-related words to image tokens.\n",
    "#\n",
    "#     Args:\n",
    "#         attention_maps_per_step: List of attention maps extracted from OctoModel.\n",
    "#         task_token_idx: Index of the task-related word token.\n",
    "#         image_token_indices: Indices of the image tokens.\n",
    "#\n",
    "#     Returns:\n",
    "#         A list of extracted attention scores mapping task words to image tokens.\n",
    "#     \"\"\"\n",
    "#     extracted_attention = []\n",
    "#\n",
    "#     for step in tqdm(range(len(attention_maps_per_step)), desc=\"Extracting Task-Image Attention\"):\n",
    "#         step_attention = []\n",
    "#         num_layers = len(attention_maps_per_step[step][\"all\"])\n",
    "#\n",
    "#         for layer_idx in range(num_layers):\n",
    "#             attn_map = attention_maps_per_step[step][\"all\"][layer_idx]  # Shape: (num_heads, num_tokens)\n",
    "#\n",
    "#             # Fix batch dimension issue if necessary\n",
    "#             if attn_map.ndim == 3 and attn_map.shape[0] == 1:\n",
    "#                 attn_map = attn_map[0]  # Remove batch dim\n",
    "#\n",
    "#             if attn_map.ndim != 2:\n",
    "#                 print(f\"Unexpected shape at step {step}, layer {layer_idx}: {attn_map.shape}\")\n",
    "#                 continue\n",
    "#\n",
    "#             # Extract attention from task token to image tokens\n",
    "#             task_to_image_attention = attn_map[:, task_token_idx, image_token_indices]  # Shape: (num_heads, image_token_count)\n",
    "#             step_attention.append(task_to_image_attention)\n",
    "#\n",
    "#         extracted_attention.append(step_attention)\n",
    "#\n",
    "#     return extracted_attention\n",
    "#\n",
    "# def plot_task_image_attention(attention_scores, image, title=\"Task to Image Attention\", save_path=None):\n",
    "#     \"\"\"\n",
    "#     Visualizes attention from task tokens to image tokens as a heatmap.\n",
    "#\n",
    "#     Args:\n",
    "#         attention_scores: (num_heads, image_token_count) - Extracted attention scores.\n",
    "#         image: Original image.\n",
    "#         title: Title for the visualization.\n",
    "#         save_path: If provided, saves the heatmap.\n",
    "#     \"\"\"\n",
    "#     avg_attention = attention_scores.mean(axis=0)  # Average across heads\n",
    "#     grid_size = int(np.sqrt(len(avg_attention)))\n",
    "#\n",
    "#     attn_grid = avg_attention.reshape((grid_size, grid_size))\n",
    "#     attn_grid = zoom(attn_grid, (image.shape[0] // grid_size, image.shape[1] // grid_size), order=1)\n",
    "#\n",
    "#     plt.figure(figsize=(8, 8))\n",
    "#     plt.imshow(image)\n",
    "#     sns.heatmap(attn_grid, alpha=0.6, cmap='jet', linewidths=0, linecolor='black')\n",
    "#     plt.title(title)\n",
    "#     plt.axis(\"off\")\n",
    "#\n",
    "#     if save_path:\n",
    "#         plt.savefig(save_path)\n",
    "#     plt.show()\n",
    "#\n",
    "# def run_task_image_attention_analysis(attention_maps_per_step, images, task_token_idx, image_token_indices, save_dir=\"task_image_attention\"):\n",
    "#     \"\"\"\n",
    "#     Extracts and visualizes attention from task-related words to image tokens.\n",
    "#\n",
    "#     Args:\n",
    "#         attention_maps_per_step: List of attention maps.\n",
    "#         images: List of input images.\n",
    "#         task_token_idx: Index of task-related word.\n",
    "#         image_token_indices: Indices of image tokens.\n",
    "#         save_dir: Directory to save attention visualizations.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     extracted_attention = extract_task_to_image_attention(attention_maps_per_step, task_token_idx, image_token_indices)\n",
    "#\n",
    "#     for step in range(len(images)):\n",
    "#         save_path = os.path.join(save_dir, f\"step_{step}_task_to_image.png\")\n",
    "#         plot_task_image_attention(extracted_attention[step][-1], images[step], title=f\"Step {step} - Task to Image Attention\", save_path=save_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
