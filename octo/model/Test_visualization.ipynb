{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-02-23T16:24:30.252866Z",
     "end_time": "2025-02-23T16:24:42.449235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e34f0e228614cb494d3e4caf1c92694"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "from octo.model.octo_model import OctoModel\n",
    "model = OctoModel.load_pretrained(\"hf://rail-berkeley/octo-small-1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BRIDGE dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:27:03.125628: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Place the can to the left of the pot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:27:11.592 python[34428:2606592] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-23 16:27:11.592 python[34428:2606592] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_datasets as tfds\n",
    "import cv2\n",
    "import jax\n",
    "from PIL import Image\n",
    "import mediapy as mp\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "### Load the BRIDGE Dataset\n",
    "print(\"Loading BRIDGE dataset...\")\n",
    "builder = tfds.builder_from_directory(builder_dir=\"gs://gresearch/robotics/bridge/0.1.0/\")\n",
    "ds = builder.as_dataset(split=\"train[:1]\")  # Load first episode\n",
    "\n",
    "# Extract a single episode\n",
    "episode = next(iter(ds))\n",
    "steps = list(episode[\"steps\"])\n",
    "\n",
    "# Resize images to 256x256 (default for Octo model)\n",
    "images = [cv2.resize(np.array(step[\"observation\"][\"image\"]), (256, 256)) for step in steps]\n",
    "\n",
    "# Extract goal image (last frame) & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[0][\"observation\"][\"natural_language_instruction\"].numpy().decode()\n",
    "\n",
    "print(f\"Instruction: {language_instruction}\")\n",
    "for img in images:\n",
    "    cv2.imshow(\"Episode Frame\", img)\n",
    "    cv2.waitKey(100)  # Wait 100ms per frame\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-23T16:27:03.111935Z",
     "end_time": "2025-02-23T16:27:15.253956Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Octo model checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71e79e38bee741be9b0e6958761b258f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/timestep', 'timestep', 'image_wrist', 'pad_mask_dict/image_wrist'}\n"
     ]
    }
   ],
   "source": [
    "### Load the Pretrained Octo Model\n",
    "print(\"Loading Octo model checkpoint...\")\n",
    "model = OctoModel.load_pretrained(\"hf://rail-berkeley/octo-base-1.5\")\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "task = model.create_tasks(goals={\"image_primary\": goal_image[None]})   # for goal-conditioned\n",
    "task = model.create_tasks(texts=[language_instruction])                # for language conditioned"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-23T16:27:28.731866Z",
     "end_time": "2025-02-23T16:27:41.000258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]WARNING:root:'observations' is missing items compared to example_batch: {'pad_mask_dict/timestep', 'pad_mask_dict/image_primary', 'image_wrist', 'pad_mask_dict/image_wrist', 'timestep', 'task_completed'}\n",
      "WARNING:root:No pad_mask_dict found. Nothing will be masked.\n",
      "WARNING:root:Skipping observation tokenizer: obs_wrist\n",
      "100%|██████████| 37/37 [00:26<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_actions, true_actions, attention_maps_per_step = [], [], []\n",
    "for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "    input_images = np.stack(images[step:step+WINDOW_SIZE])[None]\n",
    "    observation = {\n",
    "        'image_primary': input_images,\n",
    "        'timestep_pad_mask': np.full((1, input_images.shape[1]), True, dtype=bool)\n",
    "    }\n",
    "\n",
    "    # Get both predicted actions and attention maps\n",
    "    actions, attention_maps = model.sample_actions(\n",
    "        observation,\n",
    "        task,\n",
    "        unnormalization_statistics=model.dataset_statistics[\"bridge_dataset\"][\"action\"],\n",
    "        rng=jax.random.PRNGKey(0)\n",
    "    )\n",
    "\n",
    "    # Store predicted actions\n",
    "    pred_actions.append(actions[0])\n",
    "\n",
    "    # Store attention maps (for all layers & heads at this step)\n",
    "    attention_maps_per_step.append(attention_maps)\n",
    "\n",
    "    # Store true actions\n",
    "    final_window_step = step + WINDOW_SIZE - 1\n",
    "    true_actions.append(np.concatenate(\n",
    "        (\n",
    "            steps[final_window_step]['action']['world_vector'],\n",
    "            steps[final_window_step]['action']['rotation_delta'],\n",
    "            np.array(steps[final_window_step]['action']['open_gripper']).astype(np.float32)[None]\n",
    "        ), axis=-1\n",
    "    ))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-23T16:27:55.376874Z",
     "end_time": "2025-02-23T16:28:21.877956Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def overlay_attention_on_image(image, attn_map, patch_size=16):\n",
    "    \"\"\"\n",
    "    Overlays attention heatmap on an input image.\n",
    "\n",
    "    Args:\n",
    "        image: (H, W, C) - The original image.\n",
    "        attn_map: (num_heads, num_tokens) - Attention scores for image tokens.\n",
    "        patch_size: The patch size used in tokenization (default = 16).\n",
    "\n",
    "    Returns:\n",
    "        List of overlayed images (one per head).\n",
    "    \"\"\"\n",
    "    num_heads, num_patches = attn_map.shape\n",
    "    grid_size = int(np.sqrt(num_patches))  # E.g., 256 tokens → 16x16 grid\n",
    "    overlayed_images = []\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        # Normalize attention map per head\n",
    "        attn_grid = attn_map[head].reshape((grid_size, grid_size))\n",
    "        attn_grid = (attn_grid - attn_grid.min()) / (attn_grid.max() - attn_grid.min())\n",
    "\n",
    "        # Upsample the attention map to match image size\n",
    "        attn_heatmap = zoom(attn_grid, (patch_size, patch_size), order=1)\n",
    "\n",
    "        # Convert attention map to heatmap\n",
    "        attn_colormap = cv2.applyColorMap((attn_heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "        # Resize the heatmap to match the image resolution\n",
    "        attn_colormap = cv2.resize(attn_colormap, (image.shape[1], image.shape[0]))\n",
    "\n",
    "        # Blend the heatmap with the original image\n",
    "        overlayed_image = cv2.addWeighted(image, 0.6, attn_colormap, 0.4, 0)\n",
    "        overlayed_images.append(overlayed_image)\n",
    "\n",
    "    return overlayed_images  # List of images, one per head"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-23T16:29:07.158169Z",
     "end_time": "2025-02-23T16:29:07.160959Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def visualize_attention_maps(attention_maps_per_step, images, save_path=\"attention_visualization\"):\n",
    "    \"\"\"\n",
    "    Visualizes and saves attention maps overlaid on images.\n",
    "\n",
    "    Args:\n",
    "        attention_maps_per_step: List of attention maps extracted from OctoModel.\n",
    "        images: List of original images used for inference.\n",
    "        save_path: Directory to save visualized attention maps.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    num_steps = len(attention_maps_per_step)\n",
    "    num_layers = len(attention_maps_per_step[0][\"all\"])  # Assuming all steps have same layers\n",
    "\n",
    "    for step in tqdm.trange(num_steps, desc=\"Visualizing Attention\"):\n",
    "        image = images[step]\n",
    "\n",
    "        for layer_idx in range(num_layers):  # Iterate over all layers\n",
    "            attn_map = attention_maps_per_step[step][\"all\"][layer_idx][0] # Shape: (num_heads, image_token_num)\n",
    "\n",
    "            # Verify shape is correct\n",
    "            if attn_map.ndim != 2:\n",
    "                print(f\"Unexpected shape for attention map at step {step}, layer {layer_idx}: {attn_map.shape}\")\n",
    "                continue\n",
    "\n",
    "            # Get overlaid images per head\n",
    "            overlayed_images = overlay_attention_on_image(image, attn_map)\n",
    "\n",
    "            # Plot all attention heads in a row\n",
    "            num_heads = attn_map.shape[0]\n",
    "            fig, axes = plt.subplots(1, num_heads, figsize=(20, 5))\n",
    "\n",
    "            for head_idx in range(num_heads):\n",
    "                axes[head_idx].imshow(overlayed_images[head_idx])\n",
    "                axes[head_idx].set_title(f\"Layer {layer_idx} - Head {head_idx}\")\n",
    "                axes[head_idx].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(save_path, f\"step_{step}_layer_{layer_idx}.png\"))\n",
    "            plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-23T16:33:47.923652Z",
     "end_time": "2025-02-23T16:33:47.927091Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing Attention: 100%|██████████| 37/37 [03:32<00:00,  5.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run visualization function\n",
    "visualize_attention_maps(attention_maps_per_step, images, save_path=\"attention_results\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-23T16:33:50.643344Z",
     "end_time": "2025-02-23T16:37:23.605850Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
