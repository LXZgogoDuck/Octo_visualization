{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:32:23.471349Z",
     "end_time": "2025-03-06T07:32:23.473564Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import jax\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "from octo.model.octo_model import OctoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Octo model checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c3d71d59dc54c8cb7c706e8f6c7ab19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Octo model\n",
    "print(\"Loading Octo model checkpoint...\")\n",
    "model = OctoModel.load_pretrained(\"hf://rail-berkeley/octo-base-1.5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:32:23.475777Z",
     "end_time": "2025-03-06T07:32:30.151506Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Load multiple episodes from BRIDGE dataset\n",
    "dataset_split = \"train[:100]\"  # Adjust number of episodes as needed\n",
    "builder = tfds.builder_from_directory(builder_dir=\"gs://gresearch/robotics/bridge/0.1.0/\")\n",
    "ds = builder.as_dataset(split=dataset_split)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T11:35:43.040547Z",
     "end_time": "2025-03-06T11:35:44.935855Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Episode 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]WARNING:root:'observations' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/image_primary', 'pad_mask_dict/timestep', 'task_completed'}\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n",
      "WARNING:root:No pad_mask_dict found. Nothing will be masked.\n",
      "WARNING:root:Skipping observation tokenizer: obs_wrist\n",
      "100%|██████████| 37/37 [00:24<00:00,  1.51it/s]\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_1.mp4\n",
      "Saved instruction: episode_1_instruction.txt\n",
      "Processing Episode 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:17<00:00,  1.77it/s]\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_2.mp4\n",
      "Saved instruction: episode_2_instruction.txt\n",
      "Processing Episode 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:22<00:00,  1.40it/s]\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_3.mp4\n",
      "Saved instruction: episode_3_instruction.txt\n",
      "Processing Episode 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:14<00:00,  1.48it/s]\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_4.mp4\n",
      "Saved instruction: episode_4_instruction.txt\n",
      "Processing Episode 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:25<00:00,  1.85it/s]\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_5.mp4\n",
      "Saved instruction: episode_5_instruction.txt\n",
      "Processing Episode 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:12<00:00,  2.10it/s]\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_6.mp4\n",
      "Saved instruction: episode_6_instruction.txt\n",
      "Processing Episode 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:13<00:00,  2.28it/s]\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_7.mp4\n",
      "Saved instruction: episode_7_instruction.txt\n",
      "Processing Episode 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:21<00:00,  2.12it/s]\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_8.mp4\n",
      "Saved instruction: episode_8_instruction.txt\n",
      "Processing Episode 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:09<00:00,  2.18it/s]\n",
      "WARNING:root:'tasks' is missing items compared to example_batch: {'pad_mask_dict/image_wrist', 'timestep', 'image_wrist', 'pad_mask_dict/timestep'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_9.mp4\n",
      "Saved instruction: episode_9_instruction.txt\n",
      "Processing Episode 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:23<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training video: training_videos/episode_10.mp4\n",
      "Saved instruction: episode_10_instruction.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 2\n",
    "num_episodes = 20\n",
    "\n",
    "all_pred_actions = []\n",
    "all_true_actions = []\n",
    "all_attention_maps = []\n",
    "all_images = []\n",
    "\n",
    "output_dir = \"training_videos_1-10\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for episode_idx, episode in enumerate(tfds.as_numpy(ds.take(num_episodes))):\n",
    "    print(f\"Processing Episode {episode_idx+1}/{num_episodes}\")\n",
    "\n",
    "    steps = list(episode[\"steps\"])\n",
    "    images = [cv2.resize(np.array(step[\"observation\"][\"image\"]), (256, 256)) for step in steps]\n",
    "    goal_image = images[-1]\n",
    "    language_instruction = steps[0][\"observation\"][\"natural_language_instruction\"].decode()\n",
    "    all_images.append(images)\n",
    "\n",
    "    task = model.create_tasks(goals={\"image_primary\": goal_image[None]})\n",
    "    pred_actions, true_actions, attention_maps_per_step = [], [], []\n",
    "\n",
    "    video_filename = os.path.join(output_dir, f\"episode_{episode_idx+1}.mp4\")\n",
    "    video_writer = cv2.VideoWriter(video_filename, cv2.VideoWriter_fourcc(*\"mp4v\"), 5, (256, 256))\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"episode_{episode_idx+1}_instruction.txt\"), \"w\") as f:\n",
    "        f.write(language_instruction)\n",
    "\n",
    "    for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "        input_images = np.stack(images[step:step+WINDOW_SIZE])[None]\n",
    "        observation = {\n",
    "            'image_primary': input_images,\n",
    "            'timestep_pad_mask': np.full((1, input_images.shape[1]), True, dtype=bool)\n",
    "        }\n",
    "\n",
    "        actions, attention_maps = model.sample_actions(\n",
    "            observation,\n",
    "            task,\n",
    "            unnormalization_statistics=model.dataset_statistics[\"bridge_dataset\"][\"action\"],\n",
    "            rng=jax.random.PRNGKey(0)\n",
    "        )\n",
    "\n",
    "        pred_actions.append(actions[0])\n",
    "        attention_maps_per_step.append(attention_maps)\n",
    "\n",
    "        final_window_step = step + WINDOW_SIZE - 1\n",
    "        true_actions.append(np.concatenate(\n",
    "            (\n",
    "                steps[final_window_step]['action']['world_vector'],\n",
    "                steps[final_window_step]['action']['rotation_delta'],\n",
    "                np.array(steps[final_window_step]['action']['open_gripper']).astype(np.float32)[None]\n",
    "            ), axis=-1\n",
    "        ))\n",
    "\n",
    "        frame = images[step]\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Saved training video: {video_filename}\")\n",
    "    print(f\"Saved instruction: episode_{episode_idx+1}_instruction.txt\")\n",
    "\n",
    "    all_pred_actions.append(pred_actions)\n",
    "    all_true_actions.append(true_actions)\n",
    "    all_attention_maps.append(attention_maps_per_step)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:32:31.925016Z",
     "end_time": "2025-03-06T07:35:40.653407Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def overlay_attention_on_image(image, attn_map, patch_size=16):\n",
    "    num_heads, num_patches = attn_map.shape\n",
    "    grid_size = int(np.sqrt(num_patches))\n",
    "    overlayed_images = []\n",
    "\n",
    "    for head in range(num_heads):\n",
    "        attn_grid = attn_map[head].reshape((grid_size, grid_size))\n",
    "        attn_grid = (attn_grid - attn_grid.min()) / (attn_grid.max() - attn_grid.min())\n",
    "        attn_heatmap = zoom(attn_grid, (patch_size, patch_size), order=1)\n",
    "        attn_colormap = cv2.applyColorMap((attn_heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        attn_colormap = cv2.resize(attn_colormap, (image.shape[1], image.shape[0]))\n",
    "        overlayed_image = cv2.addWeighted(image, 0.6, attn_colormap, 0.4, 0)\n",
    "        overlayed_images.append(overlayed_image)\n",
    "    return overlayed_images"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:35:40.679652Z",
     "end_time": "2025-03-06T07:35:40.703347Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interpolation (Smoothing):\n",
    "\n",
    "zoom(attn_grid, (patch_size, patch_size), order=1) upscales the coarse attention map using bilinear interpolation (order=1), which makes the attention map appear smoother rather than blocky.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing Episode 1: 100%|██████████| 37/37 [01:03<00:00,  1.72s/it]\n",
      "Visualizing Episode 2: 100%|██████████| 31/31 [01:37<00:00,  3.15s/it]\n",
      "Visualizing Episode 3: 100%|██████████| 31/31 [00:54<00:00,  1.75s/it]\n",
      "Visualizing Episode 4: 100%|██████████| 21/21 [00:37<00:00,  1.76s/it]\n",
      "Visualizing Episode 5: 100%|██████████| 47/47 [01:23<00:00,  1.78s/it]\n",
      "Visualizing Episode 6: 100%|██████████| 26/26 [00:50<00:00,  1.95s/it]\n",
      "Visualizing Episode 7: 100%|██████████| 31/31 [01:05<00:00,  2.12s/it]\n",
      "Visualizing Episode 8: 100%|██████████| 46/46 [01:17<00:00,  1.68s/it]\n",
      "Visualizing Episode 9: 100%|██████████| 21/21 [00:51<00:00,  2.46s/it]\n",
      "Visualizing Episode 10: 100%|██████████| 48/48 [01:18<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "def visualize_attention_episodes(all_attention_maps, all_images, save_path=\"attention_results\", last_n_layers=5):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    for episode_idx, attention_maps_per_step in enumerate(all_attention_maps):\n",
    "        episode_save_path = os.path.join(save_path, f\"episode_{episode_idx+1}\")\n",
    "        os.makedirs(episode_save_path, exist_ok=True)  # Create a directory for each episode\n",
    "\n",
    "        images = all_images[episode_idx]\n",
    "        num_steps = len(attention_maps_per_step)\n",
    "        num_layers = len(attention_maps_per_step[0][\"all\"])\n",
    "\n",
    "        for step in tqdm.trange(num_steps, desc=f\"Visualizing Episode {episode_idx+1}\"):\n",
    "            image = images[step]\n",
    "\n",
    "            for layer_idx in range(num_layers - last_n_layers, num_layers):\n",
    "                layer_save_path = os.path.join(episode_save_path, f\"layer_{layer_idx}\")\n",
    "                os.makedirs(layer_save_path, exist_ok=True)  # Create sub-folder for each layer in the episode\n",
    "\n",
    "                attn_map = attention_maps_per_step[step][\"all\"][layer_idx][0]\n",
    "                if attn_map.ndim != 2:\n",
    "                    continue\n",
    "\n",
    "                overlayed_images = overlay_attention_on_image(image, attn_map)\n",
    "                num_heads = attn_map.shape[0]\n",
    "                fig, axes = plt.subplots(1, num_heads, figsize=(20, 5))\n",
    "\n",
    "                for head_idx in range(num_heads):\n",
    "                    axes[head_idx].imshow(overlayed_images[head_idx])\n",
    "                    axes[head_idx].set_title(f\"Ep {episode_idx+1}, Step {step}, L{layer_idx}, H{head_idx}\")\n",
    "                    axes[head_idx].axis(\"off\")\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(layer_save_path, f\"step_{step}.png\"))\n",
    "                plt.close()\n",
    "\n",
    "visualize_attention_episodes(all_attention_maps, all_images)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:35:40.700002Z",
     "end_time": "2025-03-06T07:46:40.272741Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating videos for episode_1...\n",
      "Video saved: attention_results/episode_1/layer_10new.mp4\n",
      "Video saved: attention_results/episode_1/layer_11new.mp4\n",
      "Video saved: attention_results/episode_1/layer_7new.mp4\n",
      "Video saved: attention_results/episode_1/layer_8new.mp4\n",
      "Video saved: attention_results/episode_1/layer_9new.mp4\n",
      "Generating videos for episode_10...\n",
      "Video saved: attention_results/episode_10/layer_10new.mp4\n",
      "Video saved: attention_results/episode_10/layer_11new.mp4\n",
      "Video saved: attention_results/episode_10/layer_7new.mp4\n",
      "Video saved: attention_results/episode_10/layer_8new.mp4\n",
      "Video saved: attention_results/episode_10/layer_9new.mp4\n",
      "Generating videos for episode_2...\n",
      "Video saved: attention_results/episode_2/layer_10new.mp4\n",
      "Video saved: attention_results/episode_2/layer_11new.mp4\n",
      "Video saved: attention_results/episode_2/layer_7new.mp4\n",
      "Video saved: attention_results/episode_2/layer_8new.mp4\n",
      "Video saved: attention_results/episode_2/layer_9new.mp4\n",
      "Generating videos for episode_3...\n",
      "Video saved: attention_results/episode_3/layer_10new.mp4\n",
      "Video saved: attention_results/episode_3/layer_11new.mp4\n",
      "Video saved: attention_results/episode_3/layer_7new.mp4\n",
      "Video saved: attention_results/episode_3/layer_8new.mp4\n",
      "Video saved: attention_results/episode_3/layer_9new.mp4\n",
      "Generating videos for episode_4...\n",
      "Video saved: attention_results/episode_4/layer_10new.mp4\n",
      "Video saved: attention_results/episode_4/layer_11new.mp4\n",
      "Video saved: attention_results/episode_4/layer_7new.mp4\n",
      "Video saved: attention_results/episode_4/layer_8new.mp4\n",
      "Video saved: attention_results/episode_4/layer_9new.mp4\n",
      "Generating videos for episode_5...\n",
      "Video saved: attention_results/episode_5/layer_10new.mp4\n",
      "Video saved: attention_results/episode_5/layer_11new.mp4\n",
      "Video saved: attention_results/episode_5/layer_7new.mp4\n",
      "Video saved: attention_results/episode_5/layer_8new.mp4\n",
      "Video saved: attention_results/episode_5/layer_9new.mp4\n",
      "Generating videos for episode_6...\n",
      "Video saved: attention_results/episode_6/layer_10new.mp4\n",
      "Video saved: attention_results/episode_6/layer_11new.mp4\n",
      "Video saved: attention_results/episode_6/layer_7new.mp4\n",
      "Video saved: attention_results/episode_6/layer_8new.mp4\n",
      "Video saved: attention_results/episode_6/layer_9new.mp4\n",
      "Generating videos for episode_7...\n",
      "Video saved: attention_results/episode_7/layer_10new.mp4\n",
      "Video saved: attention_results/episode_7/layer_11new.mp4\n",
      "Video saved: attention_results/episode_7/layer_7new.mp4\n",
      "Video saved: attention_results/episode_7/layer_8new.mp4\n",
      "Video saved: attention_results/episode_7/layer_9new.mp4\n",
      "Generating videos for episode_8...\n",
      "Video saved: attention_results/episode_8/layer_10new.mp4\n",
      "Video saved: attention_results/episode_8/layer_11new.mp4\n",
      "Video saved: attention_results/episode_8/layer_7new.mp4\n",
      "Video saved: attention_results/episode_8/layer_8new.mp4\n",
      "Video saved: attention_results/episode_8/layer_9new.mp4\n",
      "Generating videos for episode_9...\n",
      "Video saved: attention_results/episode_9/layer_10new.mp4\n",
      "Video saved: attention_results/episode_9/layer_11new.mp4\n",
      "Video saved: attention_results/episode_9/layer_7new.mp4\n",
      "Video saved: attention_results/episode_9/layer_8new.mp4\n",
      "Video saved: attention_results/episode_9/layer_9new.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "def create_video_from_images(image_folder, video_path, fps=5):\n",
    "    \"\"\"\n",
    "    Creates a video from images stored in a folder.\n",
    "\n",
    "    Args:\n",
    "        image_folder (str): Path to the folder containing images.\n",
    "        video_path (str): Path to save the output video file.\n",
    "        fps (int): Frames per second.\n",
    "    \"\"\"\n",
    "    images = sorted([img for img in os.listdir(image_folder) if img.endswith(\".png\")], key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "\n",
    "    if not images:\n",
    "        print(f\"No images found in {image_folder}, skipping video creation.\")\n",
    "        return\n",
    "\n",
    "    first_image = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "    height, width, _ = first_image.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "    video = cv2.VideoWriter(video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for image in images:\n",
    "        img_path = os.path.join(image_folder, image)\n",
    "        frame = cv2.imread(img_path)\n",
    "        video.write(frame)\n",
    "\n",
    "    video.release()\n",
    "    print(f\"Video saved: {video_path}\")\n",
    "\n",
    "def generate_videos_for_layers(save_path=\"attention_results\"):\n",
    "    \"\"\"\n",
    "    Generates videos for each layer across all episodes.\n",
    "\n",
    "    Args:\n",
    "        save_path (str): The root folder containing episode directories.\n",
    "    \"\"\"\n",
    "    for episode_folder in sorted(os.listdir(save_path)):\n",
    "        episode_path = os.path.join(save_path, episode_folder)\n",
    "\n",
    "        if not os.path.isdir(episode_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating videos for {episode_folder}...\")\n",
    "\n",
    "        for layer_folder in sorted(os.listdir(episode_path)):\n",
    "            layer_path = os.path.join(episode_path, layer_folder)\n",
    "\n",
    "            if not os.path.isdir(layer_path):\n",
    "                continue\n",
    "\n",
    "            video_path = os.path.join(episode_path, f\"{layer_folder}new.mp4\")\n",
    "            create_video_from_images(layer_path, video_path)\n",
    "\n",
    "generate_videos_for_layers()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:46:40.262593Z",
     "end_time": "2025-03-06T07:47:10.303005Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating combined video: attention_results/episode_1/episode_1_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_1/episode_1_combined.mp4\n",
      "Creating combined video: attention_results/episode_10/episode_10_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_10/episode_10_combined.mp4\n",
      "Creating combined video: attention_results/episode_2/episode_2_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_2/episode_2_combined.mp4\n",
      "Creating combined video: attention_results/episode_3/episode_3_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_3/episode_3_combined.mp4\n",
      "Creating combined video: attention_results/episode_4/episode_4_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_4/episode_4_combined.mp4\n",
      "Creating combined video: attention_results/episode_5/episode_5_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_5/episode_5_combined.mp4\n",
      "Creating combined video: attention_results/episode_6/episode_6_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_6/episode_6_combined.mp4\n",
      "Creating combined video: attention_results/episode_7/episode_7_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_7/episode_7_combined.mp4\n",
      "Creating combined video: attention_results/episode_8/episode_8_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_8/episode_8_combined.mp4\n",
      "Creating combined video: attention_results/episode_9/episode_9_combined.mp4\n",
      "Combined episode video saved: attention_results/episode_9/episode_9_combined.mp4\n"
     ]
    }
   ],
   "source": [
    "def combine_videos_for_episode(episode_path, output_video_path, fps=2):\n",
    "    \"\"\"\n",
    "    Combines multiple layer videos into a single episode video by stacking frames vertically.\n",
    "\n",
    "    Args:\n",
    "        episode_path (str): Path to the episode folder containing layer videos.\n",
    "        output_video_path (str): Path to save the final combined episode video.\n",
    "        fps (int): Frames per second.\n",
    "    \"\"\"\n",
    "    layer_videos = sorted([vid for vid in os.listdir(episode_path) if vid.endswith(\".mp4\")])\n",
    "\n",
    "    if not layer_videos:\n",
    "        print(f\"No layer videos found in {episode_path}, skipping.\")\n",
    "        return\n",
    "\n",
    "    # Open video captures for each layer\n",
    "    video_caps = [cv2.VideoCapture(os.path.join(episode_path, vid)) for vid in layer_videos]\n",
    "\n",
    "    # Get frame width & height from the first video\n",
    "    success, first_frame = video_caps[0].read()\n",
    "    if not success:\n",
    "        print(f\"Error reading first frame from {layer_videos[0]}\")\n",
    "        return\n",
    "\n",
    "    frame_height, frame_width, _ = first_frame.shape\n",
    "    num_layers = len(video_caps)\n",
    "\n",
    "    # Define output video size (stacking layers vertically)\n",
    "    combined_height = frame_height * num_layers\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, combined_height))\n",
    "\n",
    "    print(f\"Creating combined video: {output_video_path}\")\n",
    "\n",
    "    while True:\n",
    "        frames = []\n",
    "        for cap in video_caps:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "\n",
    "        if len(frames) != num_layers:  # Stop when any layer runs out of frames\n",
    "            break\n",
    "\n",
    "        combined_frame = np.vstack(frames)  # Stack frames vertically\n",
    "        output_video.write(combined_frame)\n",
    "\n",
    "    # Release all resources\n",
    "    for cap in video_caps:\n",
    "        cap.release()\n",
    "    output_video.release()\n",
    "    print(f\"Combined episode video saved: {output_video_path}\")\n",
    "\n",
    "def generate_combined_videos(save_path=\"attention_results\"):\n",
    "    \"\"\"\n",
    "    Generates a combined video for each episode by stacking layer videos vertically.\n",
    "\n",
    "    Args:\n",
    "        save_path (str): The root folder containing episode directories.\n",
    "    \"\"\"\n",
    "    for episode_folder in sorted(os.listdir(save_path)):\n",
    "        episode_path = os.path.join(save_path, episode_folder)\n",
    "\n",
    "        if not os.path.isdir(episode_path):\n",
    "            continue\n",
    "\n",
    "        output_video_path = os.path.join(episode_path, f\"{episode_folder}_combined.mp4\")\n",
    "        combine_videos_for_episode(episode_path, output_video_path)\n",
    "\n",
    "# Run the function to generate combined videos\n",
    "generate_combined_videos(\"attention_results\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:47:10.314691Z",
     "end_time": "2025-03-06T07:47:20.251824Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_1: L2 Distance: 0.123, Cosine Similarity: 0.938\n",
      "episode_2: L2 Distance: 0.085, Cosine Similarity: 0.600\n",
      "episode_3: L2 Distance: 0.074, Cosine Similarity: 0.997\n",
      "episode_4: L2 Distance: 0.100, Cosine Similarity: 0.679\n",
      "episode_5: L2 Distance: 0.295, Cosine Similarity: 0.534\n",
      "episode_6: L2 Distance: 0.301, Cosine Similarity: 0.445\n",
      "episode_7: L2 Distance: 0.068, Cosine Similarity: 0.997\n",
      "episode_8: L2 Distance: 0.173, Cosine Similarity: 0.628\n",
      "episode_9: L2 Distance: 0.075, Cosine Similarity: 0.677\n",
      "episode_10: L2 Distance: 0.273, Cosine Similarity: 0.504\n",
      "episode_1: Success\n",
      "episode_2: Success\n",
      "episode_3: Success\n",
      "episode_4: Success\n",
      "episode_5: Failure\n",
      "episode_6: Failure\n",
      "episode_7: Success\n",
      "episode_8: Failure\n",
      "episode_9: Success\n",
      "episode_10: Failure\n",
      "Overall success rate: 60.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_action_accuracy(pred_actions, true_actions):\n",
    "    \"\"\"\n",
    "    Evaluates the quality of predicted actions using L2 distance and cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        pred_actions (list of np.array): List of predicted actions.\n",
    "        true_actions (list of np.array): List of ground truth actions.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing L2 distance and cosine similarity per episode.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for episode_idx, (pred, true) in enumerate(zip(pred_actions, true_actions)):\n",
    "        pred = np.array(pred)  # Shape: (num_steps, action_horizon, action_dim)\n",
    "        true = np.array(true)  # Shape: (num_steps, action_dim)\n",
    "\n",
    "        if pred.ndim == 3:  # If prediction has action horizon, reduce dimension\n",
    "            pred = pred[:, 0, :]  # Option 1: Take the first action per step\n",
    "            # pred = pred.mean(axis=1)  # Option 2: Average over horizon\n",
    "\n",
    "        if pred.shape != true.shape:\n",
    "            print(f\"Warning: Shape mismatch in Episode {episode_idx+1}. Pred: {pred.shape}, True: {true.shape}\")\n",
    "            continue  # Skip problematic episodes\n",
    "\n",
    "        l2_dist = np.linalg.norm(pred - true, axis=1)  # L2 distance per step\n",
    "        cos_sim = np.sum(pred * true, axis=1) / (np.linalg.norm(pred, axis=1) * np.linalg.norm(true, axis=1))  # Cosine similarity\n",
    "\n",
    "        results[f\"episode_{episode_idx+1}\"] = {\n",
    "            \"l2_mean\": np.mean(l2_dist),\n",
    "            \"l2_std\": np.std(l2_dist),\n",
    "            \"cosine_mean\": np.mean(cos_sim),\n",
    "            \"cosine_std\": np.std(cos_sim),\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = evaluate_action_accuracy(all_pred_actions, all_true_actions)\n",
    "for ep, metrics in results.items():\n",
    "    print(f\"{ep}: L2 Distance: {metrics['l2_mean']:.3f}, Cosine Similarity: {metrics['cosine_mean']:.3f}\")\n",
    "\n",
    "SUCCESS_THRESHOLD_L2 = 0.1  # Set threshold based on data\n",
    "SUCCESS_THRESHOLD_COS = 0.9\n",
    "\n",
    "def evaluate_success(results):\n",
    "    success_rates = []\n",
    "    for ep, metrics in results.items():\n",
    "        l2_score = metrics[\"l2_mean\"]\n",
    "        cos_score = metrics[\"cosine_mean\"]\n",
    "\n",
    "        # Weighted decision: if one is good, still count as success\n",
    "        success = (l2_score < SUCCESS_THRESHOLD_L2) or (cos_score > SUCCESS_THRESHOLD_COS)\n",
    "        success_rates.append(success)\n",
    "\n",
    "        print(f\"{ep}: {'Success' if success else 'Failure'}\")\n",
    "\n",
    "    overall_success = np.mean(success_rates) * 100\n",
    "    print(f\"Overall success rate: {overall_success:.2f}%\")\n",
    "\n",
    "evaluate_success(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:47:20.258685Z",
     "end_time": "2025-03-06T07:47:20.307070Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Episode 1/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 2/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 3/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 4/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 5/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 6/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 7/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 8/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 9/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 10/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 11/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 12/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 13/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 14/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 15/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 16/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 17/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 18/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 19/20\n",
      "Place the cheese wedge on the front right side of the table\n",
      "Processing Episode 20/20\n",
      "Place the cheese wedge on the front right side of the table\n"
     ]
    }
   ],
   "source": [
    "for episode_idx, episode in enumerate(tfds.as_numpy(ds.take(num_episodes))):\n",
    "    print(f\"Processing Episode {episode_idx+1}/{num_episodes}\")\n",
    "    language_instruction = steps[0][\"observation\"][\"natural_language_instruction\"].decode()\n",
    "    print(language_instruction)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T11:36:17.175741Z",
     "end_time": "2025-03-06T11:37:06.545082Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before, we just do offline evaluation. Now, run on simpler_env to so that we can see the true simulation and results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import jax\n",
    "# import tqdm\n",
    "# import imageio\n",
    "# import SimplerEnv  # Import SimplerEnv ?????????????\n",
    "# from octo.model.octo_model import OctoModel\n",
    "# from scipy.ndimage import zoom\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# # Load Octo Model\n",
    "# print(\"Loading Octo model checkpoint...\")\n",
    "# model = OctoModel.load_pretrained(\"hf://rail-berkeley/octo-base-1.5\")\n",
    "#\n",
    "# # Initialize SimplerEnv\n",
    "# env = SimplerEnv.make(\"SimplerEnv-v0\")  # Change to match the correct environment name\n",
    "# obs, info = env.reset()\n",
    "#\n",
    "# WINDOW_SIZE = 2  # Number of past frames as input\n",
    "# fps = 5  # Video frame rate\n",
    "# output_video = \"simpler_env_simulation.mp4\"\n",
    "# gif_output = \"simpler_env_simulation.gif\"\n",
    "# video_frames = []\n",
    "# attention_frames = []\n",
    "#\n",
    "# # Initialize observation history\n",
    "# observation_history = [obs] * WINDOW_SIZE\n",
    "#\n",
    "# def overlay_attention_on_image(image, attn_map, patch_size=16):\n",
    "#     \"\"\"\n",
    "#     Overlays attention heatmap on an input image.\n",
    "#     \"\"\"\n",
    "#     num_heads, num_patches = attn_map.shape\n",
    "#     grid_size = int(np.sqrt(num_patches))\n",
    "#     overlayed_images = []\n",
    "#\n",
    "#     for head in range(num_heads):\n",
    "#         attn_grid = attn_map[head].reshape((grid_size, grid_size))\n",
    "#         attn_grid = (attn_grid - attn_grid.min()) / (attn_grid.max() - attn_grid.min())\n",
    "#         attn_heatmap = zoom(attn_grid, (patch_size, patch_size), order=1)\n",
    "#         attn_colormap = cv2.applyColorMap((attn_heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "#         attn_colormap = cv2.resize(attn_colormap, (image.shape[1], image.shape[0]))\n",
    "#         overlayed_image = cv2.addWeighted(image, 0.6, attn_colormap, 0.4, 0)\n",
    "#         overlayed_images.append(overlayed_image)\n",
    "#\n",
    "#     return overlayed_images\n",
    "#\n",
    "# # Run Simulation Loop\n",
    "# for step in tqdm.trange(100):  # Run for 100 timesteps\n",
    "#     input_images = np.stack(observation_history)[None]\n",
    "#\n",
    "#     # Format input for Octo model\n",
    "#     observation = {\n",
    "#         'image_primary': input_images,\n",
    "#         'timestep_pad_mask': np.full((1, input_images.shape[1]), True, dtype=bool)\n",
    "#     }\n",
    "#\n",
    "#     # Predict action and attention map using Octo\n",
    "#     actions, attention_maps = model.sample_actions(\n",
    "#         observation,\n",
    "#         None,  # No explicit goal input\n",
    "#         unnormalization_statistics=None,  # Use default scaling\n",
    "#         rng=jax.random.PRNGKey(0)\n",
    "#     )\n",
    "#\n",
    "#     # Take action in SimplerEnv\n",
    "#     obs, reward, done, truncated, info = env.step(actions[0])\n",
    "#\n",
    "#     # Update observation history\n",
    "#     observation_history.pop(0)\n",
    "#     observation_history.append(obs)\n",
    "#\n",
    "#     # Render environment and store frame\n",
    "#     frame = env.render(mode=\"rgb_array\")  # Ensure SimplerEnv supports rendering\n",
    "#     video_frames.append(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "#\n",
    "#     # Overlay attention maps on the frame\n",
    "#     attn_map = attention_maps[\"all\"][-1][0]  # Last layer, first head\n",
    "#     attn_overlay = overlay_attention_on_image(frame, attn_map)[0]  # Use first head\n",
    "#     attention_frames.append(attn_overlay)\n",
    "#\n",
    "#     if done:\n",
    "#         print(f\"Episode finished at step {step}\")\n",
    "#         break\n",
    "#\n",
    "# env.close()\n",
    "#\n",
    "# # Save video without attention maps\n",
    "# imageio.mimsave(output_video, video_frames, fps=fps)\n",
    "# print(f\"✅ Saved simulation video: {output_video}\")\n",
    "#\n",
    "# # Save video with attention maps overlay\n",
    "# imageio.mimsave(gif_output, attention_frames, fps=fps)\n",
    "# print(f\"✅ Saved attention visualization video: {gif_output}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:47:20.309423Z",
     "end_time": "2025-03-06T07:47:20.310706Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-06T07:47:20.319972Z",
     "end_time": "2025-03-06T07:47:20.321445Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
